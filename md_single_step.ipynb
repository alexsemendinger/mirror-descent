{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')  # make plots a bit nicer by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_data(d_feature: int, n_samples: int, w_cov: np.ndarray = None, noise_scale: float = 0.0) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates data y = Xw + e where X is (n_samples, d_feature) and w is (d_feature, 1).\n",
    "\n",
    "    Args:\n",
    "        d_feature: Dimension of feature space.\n",
    "        n_samples: Number of samples to generate.\n",
    "        w_cov: Covariance matrix for generating w. If None, uses identity matrix.\n",
    "        noise_scale: Standard deviation of Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix of shape (n_samples, d_feature).\n",
    "        y: Target vector of shape (n_samples,).\n",
    "        w: True weight vector of shape (d_feature,).\n",
    "    \"\"\"\n",
    "    if w_cov is None:\n",
    "        w_cov = np.eye(d_feature)\n",
    "    \n",
    "    w = np.random.multivariate_normal(mean=np.zeros(d_feature), cov=w_cov)\n",
    "    X = np.random.randn(n_samples, d_feature)\n",
    "    y = X @ w + np.random.normal(scale=noise_scale, size=(n_samples,))\n",
    "    \n",
    "    return X, y.ravel(), w\n",
    "\n",
    "\n",
    "def generate_mixed_linear_data(d_feature: int, n_samples_per_w: int, n_ws: int, w_cov: np.ndarray = None, noise_scale: float = 0.0) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates data y = Xw + e for multiple w vectors.\n",
    "\n",
    "    Args:\n",
    "        d_feature: Dimension of feature space.\n",
    "        n_samples_per_w: Number of samples to generate for each w.\n",
    "        n_ws: Number of different w vectors to use.\n",
    "        w_cov: Covariance matrix for generating w. If None, uses identity matrix.\n",
    "        noise_scale: Standard deviation of Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix of shape (n_ws * n_samples_per_w, d_feature).\n",
    "        y: Target vector of shape (n_ws * n_samples_per_w,).\n",
    "        W: Matrix of true weight vectors of shape (n_ws, d_feature).\n",
    "    \"\"\"\n",
    "    if w_cov is None:\n",
    "        w_cov = np.eye(d_feature)\n",
    "    \n",
    "    W = np.random.multivariate_normal(mean=np.zeros(d_feature), cov=w_cov, size=n_ws)\n",
    "    X = np.random.randn(n_ws * n_samples_per_w, d_feature)\n",
    "    y = np.concatenate([X[i * n_samples_per_w : (i+1) * n_samples_per_w] @ W[i] for i in range(n_ws)])\n",
    "    y += np.random.normal(scale=noise_scale, size=y.shape)\n",
    "    \n",
    "    return X, y, W\n",
    "\n",
    "\n",
    "def generate_specified_linear_data(n_samples_per_w: int, w_stars: np.ndarray, noise_scale: float = 0.0) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates data y = Xw + e for specified w vectors.\n",
    "\n",
    "    Args:\n",
    "        n_samples_per_w: Number of samples to generate for each w.\n",
    "        w_stars: Array of weight vectors, shape (n_ws, d_feature).\n",
    "        noise_scale: Standard deviation of Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix of shape (n_ws * n_samples_per_w, d_feature).\n",
    "        y: Target vector of shape (n_ws * n_samples_per_w,).\n",
    "        W: Matrix of true weight vectors of shape (n_ws, d_feature).\n",
    "    \"\"\"\n",
    "    n_ws, d_feature = w_stars.shape\n",
    "    X = np.random.randn(n_ws * n_samples_per_w, d_feature)\n",
    "    y = np.concatenate([X[i*n_samples_per_w:(i+1)*n_samples_per_w] @ w_stars[i] for i in range(n_ws)])\n",
    "    y += np.random.normal(scale=noise_scale, size=y.shape)\n",
    "    \n",
    "    return X, y, w_stars\n",
    "\n",
    "\n",
    "def generate_std_basis_data(d_feature: int, n_samples: int, w: np.ndarray = None) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates data y = Xw where X consists of standard basis vectors.\n",
    "\n",
    "    Args:\n",
    "        d_feature: Dimension of feature space.\n",
    "        n_samples: Number of samples to generate (must be <= d_feature).\n",
    "        w: True weight vector. If None, randomly generated.\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix of shape (n_samples, d_feature).\n",
    "        y: Target vector of shape (n_samples,).\n",
    "        w: True weight vector of shape (d_feature,).\n",
    "    \"\"\"\n",
    "    assert n_samples <= d_feature, \"n_samples must be <= d_feature for standard basis vectors\"\n",
    "    \n",
    "    if w is None:\n",
    "        w = np.random.randn(d_feature)\n",
    "    \n",
    "    X = np.eye(d_feature)[:n_samples]\n",
    "    y = (X @ w).ravel()\n",
    "    \n",
    "    return X, y, w\n",
    "\n",
    "\n",
    "def generate_mixed_std_basis_data(d_feature: int, n_samples_per_w: int, n_ws: int, w_cov: np.ndarray = None, noise_scale: float = 0.0) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates data y = Xw + e for multiple w vectors, where X consists of standard basis vectors.\n",
    "\n",
    "    Args:\n",
    "        d_feature: Dimension of feature space.\n",
    "        n_samples_per_w: Number of samples to generate for each w (must be <= d_feature).\n",
    "        n_ws: Number of different w vectors to use.\n",
    "        w_cov: Covariance matrix for generating w. If None, uses identity matrix.\n",
    "        noise_scale: Standard deviation of Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix of shape (n_ws * n_samples_per_w, d_feature).\n",
    "        y: Target vector of shape (n_ws * n_samples_per_w,).\n",
    "        W: Matrix of true weight vectors of shape (n_ws, d_feature).\n",
    "    \"\"\"\n",
    "    assert n_samples_per_w <= d_feature, \"n_samples_per_w must be <= d_feature for standard basis vectors\"\n",
    "    \n",
    "    if w_cov is None:\n",
    "        w_cov = np.eye(d_feature)\n",
    "    \n",
    "    W = np.random.multivariate_normal(mean=np.zeros(d_feature), cov=w_cov, size=n_ws)\n",
    "    X = np.tile(np.eye(d_feature)[:n_samples_per_w], (n_ws, 1))\n",
    "    y = np.concatenate([W[i, :n_samples_per_w] for i in range(n_ws)])\n",
    "    y += np.random.normal(scale=noise_scale, size=y.shape)\n",
    "    \n",
    "    return X, y, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $Q$ parameterization: not ensuring psd or symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training loop âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirror_descent_step(w: np.ndarray, Q: np.ndarray, lr: float, x: np.ndarray, y: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Single step of mirror descent.\n",
    "    [ 7.1: this function isn't actually used right now, since the potential update computes this manually ]\n",
    "\n",
    "    Args:\n",
    "        w: Current weight vector (d_feature, 1).\n",
    "        Q: Potential matrix (d_feature, d_feature).\n",
    "        lr: Learning rate.\n",
    "        x: Feature vector (d_feature,).\n",
    "        y: Target value.\n",
    "\n",
    "    Returns:\n",
    "        Updated weight vector (d_feature,).\n",
    "    \"\"\"\n",
    "    return w - 2 * lr * (np.inner(w, x) - y) * (Q @ x)\n",
    "\n",
    "\n",
    "def crossval(w: np.ndarray, Q: np.ndarray, lr: float, X: np.ndarray, y: np.ndarray,\n",
    "             ignore_diag=True) -> float:\n",
    "    \"\"\"\n",
    "    Perform leave-one-out cross-validation.\n",
    "\n",
    "    For each (x_i, y_i) in the dataset:\n",
    "    1. \"Train\" a model with a single step of mirror descent on (x_i, y_i)\n",
    "    2. Evaluate it on the rest of the dataset\n",
    "\n",
    "    Tested against non-vectorized version, performs identically.\n",
    "\n",
    "    Args:\n",
    "        w: Initial weight vector (d_feature,).\n",
    "        Q: Potential matrix (d_feature, d_feature).\n",
    "        lr: Learning rate.\n",
    "        X: Feature matrix (n_samples, d_feature).\n",
    "        y: Target vector (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        Average loss over all i, j with i != j.\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "\n",
    "    # Compute all pairwise losses in a vectorized manner\n",
    "    errors = X @ w - y  # (n_samples,)\n",
    "    XQX = X @ Q.T @ X.T  # (n_samples, n_samples)\n",
    "    \n",
    "    # Broadcasting to compute L_ij for all i, j\n",
    "    L_squared = (errors - 2 * lr * errors[:, np.newaxis] * XQX)**2  # (n_samples, n_samples)\n",
    "    \n",
    "    # Enforcing i != j condition, if applicable\n",
    "    if ignore_diag:\n",
    "        np.fill_diagonal(L_squared, 0)\n",
    "        denom = 2 * n * (n-1)\n",
    "    else:\n",
    "        denom = 2 * n**2\n",
    "    \n",
    "    return np.sum(L_squared) / denom  # average over all L_ij\n",
    "\n",
    "\n",
    "def potential_update(w: np.ndarray, Q: np.ndarray, outer_lr: float, inner_lr: float, X: np.ndarray, y: np.ndarray,\n",
    "                     ignore_diag=True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Update the potential matrix Q based on the derivative of the cross-validation loss.\n",
    "\n",
    "    Args:\n",
    "        w: Current weight vector (d_feature,).\n",
    "        Q: Current potential matrix (d_feature, d_feature).\n",
    "        outer_lr: Learning rate for updating Q.\n",
    "        inner_lr: Learning rate for the inner mirror descent step (Î· in the formula).\n",
    "        X: Feature matrix (n_samples, d_feature).\n",
    "        y: Target vector (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        Updated potential matrix Q (d_feature, d_feature).\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    z = X @ w - y  # (n_samples,)\n",
    "    XQX = X @ Q.T @ X.T  # (n_samples, n_samples)\n",
    "    L = z - 2 * inner_lr * z[:, np.newaxis] * XQX  # L_{ij}, (n_samples, n_samples)\n",
    "    \n",
    "    # Enforce i != j condition, if applicable\n",
    "    if ignore_diag:\n",
    "        np.fill_diagonal(L, 0)\n",
    "        denom = n_samples * (n_samples - 1)\n",
    "    else:\n",
    "        denom = n_samples ** 2\n",
    "    \n",
    "    # Black magic einsum expression to combine everything\n",
    "    # Breaking it down:\n",
    "    #  Crossval derivative involves z_i, L_{ij}, and x_j x_i^T.\n",
    "    #  The indices of L are i,j\n",
    "    #  The index of z is i\n",
    "    #  The way to do \"sum over all outer products of pairs of rows\" is jk,il->kl\n",
    "    #   (this isn't so obvious, but see \"scratch work\" below to walk through.)\n",
    "    update = np.einsum('ij,i,jk,il->kl', L, z, X, X)\n",
    "    update *= -2 * inner_lr / denom\n",
    "    return Q - outer_lr * update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_potential(d, n, inner_lr, outer_lr, w0, Q0, n_iters,\n",
    "                    w_cov=None, noise_scale=0):\n",
    "    \"\"\"\n",
    "    Basic training loop.\n",
    "    Generates linear data and runs potential_update for n_iters iterations.\n",
    "\n",
    "    Returns crossvals, Qs, X, y, w_star.\n",
    "    \"\"\"\n",
    "    X, y, w_star = generate_linear_data(d, n, w_cov, noise_scale)\n",
    "\n",
    "    crossvals = np.zeros(n_iters)\n",
    "    Q = Q0.copy()\n",
    "    Qs = np.zeros((n_iters, d, d))\n",
    "    Qs[0] = Q.copy()\n",
    "    for i in range(n_iters):\n",
    "        crossvals[i] = crossval(w0, Q, outer_lr, inner_lr, X, y, ignore_diag=True)\n",
    "        Q = potential_update(w0, Q, outer_lr, inner_lr, X, y, ignore_diag=False)\n",
    "        Qs[i] = Q.copy()\n",
    "\n",
    "    return crossvals, Qs, X, y, w_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking correctness of `crossval` and `potential_update` âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test crossval and potential_update against unvectorized versions\n",
    "\n",
    "def random_experiment_setup():\n",
    "    d_feature = np.random.randint(5, 15)\n",
    "    n_samples = np.random.randint(5, 100)\n",
    "    lr = np.random.uniform(0.01, 1)\n",
    "    noise_scale = np.random.uniform(0, 1)\n",
    "\n",
    "    X, y, w_star = generate_linear_data(d_feature, n_samples, noise_scale=noise_scale)\n",
    "\n",
    "    w = np.random.randn(d_feature)\n",
    "    Q = np.random.randn(d_feature, d_feature)\n",
    "\n",
    "    return d_feature, n_samples, lr, X, y, w_star, w, Q\n",
    "\n",
    "n_tests = 1\n",
    "\n",
    "# ------------------------ Testing L_ij -----------------------------\n",
    "for _ in range(n_tests):\n",
    "    d, n, lr, X, y, w_star, w, Q = random_experiment_setup()\n",
    "    z = w @ X.T - y\n",
    "    M = X @ Q.T @ X.T\n",
    "    L = (z - 2 * lr * z[:, np.newaxis] * M)  # L[i,j] = L_{ij}  (assuming lr = 0.5)\n",
    "\n",
    "    # check that it works\n",
    "    for i in range(d):\n",
    "        for j in range(d):\n",
    "            manual_calc = z[j] - 2 * lr * z[i] * X[j].T @ Q @ X[i]\n",
    "            assert np.isclose(L[i,j], manual_calc), f\"{i,j} {L[i,j], manual_calc}\"\n",
    "print(\"L_ij passed test.\")\n",
    "\n",
    "\n",
    "# ------------------------ Testing crossval -----------------------------\n",
    "def crossval_nonvec(w, Q, lr, xs, ys):\n",
    "    \"\"\"\n",
    "    For each (x_i, y_i) in zip(xs, ys):\n",
    "    1. \"Train\" a model with a single step of mirror descent on (x_i, y_i)\n",
    "    2. Evaluate it on the rest of the dataset\n",
    "    Return the average loss over all i, j with i != j.\n",
    "    \"\"\"\n",
    "    k = len(xs)\n",
    "    def L_ij(w, Q, xi, xj, yi, yj):\n",
    "        return ( w.T @ xj - 2 * lr * (w.T @ xi - yi) * (xj.T @ Q @ xi) - yj )**2\n",
    "    value = 0\n",
    "    for i, (xi, yi) in enumerate(zip(xs, ys)):\n",
    "        for j, (xj, yj) in enumerate(zip(xs, ys)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            value += L_ij(w, Q, xi, xj, yi, yj).item()\n",
    "    return value / (2 * k * (k-1))\n",
    "\n",
    "for _ in range(n_tests):\n",
    "    # all parameters are randomized over a bunch of runs\n",
    "    d_feature, n_samples, lr, X, y, w_star, w, Q = random_experiment_setup()\n",
    "\n",
    "    # Prepare inputs for crossval_nonvec\n",
    "    xs = [X[i, :].reshape(-1, 1) for i in range(n_samples)]  # list of (d_feature, 1) arrays\n",
    "    ys = y.tolist()  # list of floats\n",
    "\n",
    "    result_nonvec = crossval_nonvec(w[:, np.newaxis], Q, lr, xs, ys)\n",
    "    result_vec = crossval(w, Q, lr, X, y)\n",
    "\n",
    "    assert np.isclose(result_nonvec, result_vec) \n",
    "print(\"crossval passed test.\")\n",
    "\n",
    "\n",
    "# ------------------------ Testing potential_update -----------------------------\n",
    "def potential_update_nonvec(w, Q, outer_lr, inner_lr, xs, ys):\n",
    "    \"\"\"\n",
    "    Derivative of the cross-validation loss (as implemented in `crossval` above)\n",
    "    with respect to the matrix Q.\n",
    "\n",
    "    Returns the updated matrix.\n",
    "    \"\"\"\n",
    "    k = len(xs)\n",
    "    def cv_derivative(w, Q, lr, xi, xj, yi, yj):\n",
    "        err = (w.T @ xi - yi).item()\n",
    "        #return (w.T @ xj - 2 * lr * err * (xi.T @ Q @ xj) - yj).item() * err * np.outer(xi, xj)\n",
    "        return (w.T @ xj - 2 * lr * err * (xj.T @ Q @ xi) - yj).item() * err * (xj @ xi.T)\n",
    "    \n",
    "    update = 0\n",
    "    for i, (xi, yi) in enumerate(zip(xs, ys)):\n",
    "        for j, (xj, yj) in enumerate(zip(xs, ys)):\n",
    "            if i != j:\n",
    "                update += cv_derivative(w, Q, inner_lr, xi, xj, yi, yj)\n",
    "    update = - 2 * inner_lr * update / (k * (k - 1))\n",
    "    return Q - outer_lr * update\n",
    "\n",
    "for _ in range(n_tests):\n",
    "    d_feature, n_samples, inner_lr, X, y, w_star, w, Q = random_experiment_setup()\n",
    "    outer_lr = np.random.uniform(0.01, 1)\n",
    "\n",
    "    xs = [X[i, :].reshape(-1, 1) for i in range(n_samples)]  # list of (d_feature, 1) arrays\n",
    "    ys = y.tolist()  # list of floats\n",
    "\n",
    "    result_nonvec = potential_update_nonvec(w, Q, outer_lr, inner_lr, xs, ys)\n",
    "    result_vec = potential_update(w, Q, outer_lr, inner_lr, X, y)\n",
    "    assert np.allclose(result_nonvec, result_vec) \n",
    "print(\"potential_update passed test.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch work: how do you vectorize \"sum of outer products of rows of X\"\n",
    "\n",
    "# i.e. sum_{i,j} np.outer(x_i, x_j)\n",
    "d, n = 5, 10\n",
    "X = np.random.randn(n,d)\n",
    "\n",
    "# sum of outer products of rows, written as a loop\n",
    "sopr = np.zeros((d,d))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        sopr += np.outer(X[j], X[i])\n",
    "\n",
    "# turn the loop into a nested list comprehension: (n,d) -> (n, n, d, d), and then sum over first two dims\n",
    "expand = np.array( [[ np.outer(X[i], X[j]) for j in range(n)] for i in range(n)] )\n",
    "sopr2 = expand.sum((0,1))\n",
    "assert np.allclose(sopr, sopr2)\n",
    "\n",
    "# turn into einsum: same expand step first, then sum over first two indices\n",
    "einsum_expand = np.einsum('ik,jl->ijkl', X, X)\n",
    "sopr3 = np.einsum('ik,jl->ijkl', X, X).sum((0,1))\n",
    "assert np.allclose(einsum_expand, expand)\n",
    "assert np.allclose(sopr, sopr3)\n",
    "\n",
    "# finally, turn into an even simpler einsum\n",
    "sopr4 = np.einsum('ik,jl->kl', X, X)\n",
    "assert np.allclose(sopr, sopr4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Known example: noiseless case, standard basis data âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_non_pd_potential_std_basis(w0: np.ndarray, w_star: np.ndarray, lr: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns matrix Q that makes the error L_[ij] = 0 for standard basis data.\n",
    "\n",
    "    Args:\n",
    "        w0: Initial weight vector (d_feature,).\n",
    "        w_star: True weight vector (d_feature,).\n",
    "        lr: Learning rate.\n",
    "\n",
    "    Returns:\n",
    "        Optimal potential matrix Q (d_feature, d_feature).\n",
    "    \"\"\"\n",
    "    d_feature = w0.shape[0]\n",
    "    Q = np.zeros((d_feature, d_feature))\n",
    "    for i in range(d_feature):\n",
    "        for j in range(d_feature):\n",
    "            Q[i, j] = ((w0[i] - w_star[i]) / (w0[j] - w_star[j])).item()\n",
    "    return Q / (2 * lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_feature, n_samples = 5, 5\n",
    "inner_lr, outer_lr = 1, 0.4\n",
    "n_potential_iterations = 3000\n",
    "\n",
    "X, y, w_star = generate_std_basis_data(d_feature, n_samples)\n",
    "w0 = np.ones((d_feature,))\n",
    "Q0 = np.random.randn(d_feature, d_feature)\n",
    "\n",
    "Q_star = optimal_non_pd_potential_std_basis(w0, w_star, inner_lr)\n",
    "\n",
    "Q = Q0.copy()\n",
    "crossvals = np.zeros(n_potential_iterations)\n",
    "Qs = np.zeros((n_potential_iterations, d_feature, d_feature))\n",
    "for i in range(n_potential_iterations):\n",
    "    crossvals[i] = crossval(w0, Q, inner_lr, X, y)\n",
    "    Qs[i] = Q\n",
    "    Q = potential_update(w0, Q, outer_lr, inner_lr, X, y, ignore_diag=False)\n",
    "\n",
    "# Plotting results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(crossvals)\n",
    "plt.title(\"Crossval loss\")\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "Q_dists = np.linalg.norm(Qs - Q_star, axis=(1, 2))\n",
    "plt.plot(Q_dists)\n",
    "plt.title(\"$d(Q, Q_\\star)$\")\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs for single $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs for mixed $w$ / recovering covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $UU^\\top$ parameterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs for single $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs for mixed $w$ / recovering covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
