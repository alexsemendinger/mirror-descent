{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.testing import assert_allclose\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')  # make plots a bit nicer by default\n",
    "\n",
    "# custom imports\n",
    "from plot_utils import plot_matrix_evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do / Questions\n",
    "\n",
    "1. Finish rewriting old code\n",
    "    * $Q$, multi-$w$\n",
    "    * $UU^\\top$, training loop\n",
    "    * $UU^\\top$, single $w$\n",
    "    * $UU^\\top$, multi-$w$\n",
    "\n",
    "2. Check that each of the above is learning properly\n",
    "    * Performs better than one step of GD\n",
    "        * on new data with the same $w_\\star$\n",
    "        * on new data with a different $w_\\star$??\n",
    "    * How consistent are learned matrices over multiple runs w/ same init?\n",
    "        * if not consistent, what properties do they share?\n",
    "\n",
    "3. What patterns are visible in learned $Q$?\n",
    "    * Eigenvalues, vectors.\n",
    "    * Low-rank $\\Sigma_w$: do you capture this subspace?\n",
    "    * Patterns in imshow\n",
    "\n",
    "4. Update potential based on whole batch rather than a single datapoint, for easier interp?\n",
    "\n",
    "5. Optimization-style objective, rather than crossval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_data(d_feature: int, n_samples: int, w_cov: np.ndarray = None, noise_scale: float = 0.0) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates data y = Xw + e where X is (n_samples, d_feature) and w is (d_feature, 1).\n",
    "\n",
    "    Args:\n",
    "        d_feature: Dimension of feature space.\n",
    "        n_samples: Number of samples to generate.\n",
    "        w_cov: Covariance matrix for generating w. If None, uses identity matrix.\n",
    "        noise_scale: Standard deviation of Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix of shape (n_samples, d_feature).\n",
    "        y: Target vector of shape (n_samples,).\n",
    "        w: True weight vector of shape (d_feature,).\n",
    "    \"\"\"\n",
    "    if w_cov is None:\n",
    "        w_cov = np.eye(d_feature)\n",
    "    \n",
    "    w = np.random.multivariate_normal(mean=np.zeros(d_feature), cov=w_cov)\n",
    "    X = np.random.randn(n_samples, d_feature)\n",
    "    y = X @ w + np.random.normal(scale=noise_scale, size=(n_samples,))\n",
    "    \n",
    "    return X, y.ravel(), w\n",
    "\n",
    "\n",
    "def generate_mixed_linear_data(d_feature: int, n_samples_per_w: int, n_ws: int, w_cov: np.ndarray = None, noise_scale: float = 0.0) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates data y = Xw + e for multiple w vectors.\n",
    "\n",
    "    Args:\n",
    "        d_feature: Dimension of feature space.\n",
    "        n_samples_per_w: Number of samples to generate for each w.\n",
    "        n_ws: Number of different w vectors to use.\n",
    "        w_cov: Covariance matrix for generating w. If None, uses identity matrix.\n",
    "        noise_scale: Standard deviation of Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix of shape (n_ws * n_samples_per_w, d_feature).\n",
    "        y: Target vector of shape (n_ws * n_samples_per_w,).\n",
    "        W: Matrix of true weight vectors of shape (n_ws, d_feature).\n",
    "    \"\"\"\n",
    "    if w_cov is None:\n",
    "        w_cov = np.eye(d_feature)\n",
    "    \n",
    "    W = np.random.multivariate_normal(mean=np.zeros(d_feature), cov=w_cov, size=n_ws)\n",
    "    X = np.random.randn(n_ws * n_samples_per_w, d_feature)\n",
    "    y = np.concatenate([X[i * n_samples_per_w : (i+1) * n_samples_per_w] @ W[i] for i in range(n_ws)])\n",
    "    y += np.random.normal(scale=noise_scale, size=y.shape)\n",
    "    \n",
    "    return X, y, W\n",
    "\n",
    "\n",
    "def generate_specified_linear_data(n_samples_per_w: int, w_stars: np.ndarray, noise_scale: float = 0.0) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates data y = Xw + e for specified w vectors.\n",
    "\n",
    "    Args:\n",
    "        n_samples_per_w: Number of samples to generate for each w.\n",
    "        w_stars: Array of weight vectors, shape (n_ws, d_feature).\n",
    "        noise_scale: Standard deviation of Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix of shape (n_ws * n_samples_per_w, d_feature).\n",
    "        y: Target vector of shape (n_ws * n_samples_per_w,).\n",
    "        W: Matrix of true weight vectors of shape (n_ws, d_feature).\n",
    "    \"\"\"\n",
    "    n_ws, d_feature = w_stars.shape\n",
    "    X = np.random.randn(n_ws * n_samples_per_w, d_feature)\n",
    "    y = np.concatenate([X[i*n_samples_per_w:(i+1)*n_samples_per_w] @ w_stars[i] for i in range(n_ws)])\n",
    "    y += np.random.normal(scale=noise_scale, size=y.shape)\n",
    "    \n",
    "    return X, y, w_stars\n",
    "\n",
    "\n",
    "def generate_std_basis_data(d_feature: int, n_samples: int, w: np.ndarray = None) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates data y = Xw where X consists of standard basis vectors.\n",
    "\n",
    "    Args:\n",
    "        d_feature: Dimension of feature space.\n",
    "        n_samples: Number of samples to generate (must be <= d_feature).\n",
    "        w: True weight vector. If None, randomly generated.\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix of shape (n_samples, d_feature).\n",
    "        y: Target vector of shape (n_samples,).\n",
    "        w: True weight vector of shape (d_feature,).\n",
    "    \"\"\"\n",
    "    assert n_samples <= d_feature, \"n_samples must be <= d_feature for standard basis vectors\"\n",
    "    \n",
    "    if w is None:\n",
    "        w = np.random.randn(d_feature)\n",
    "    \n",
    "    X = np.eye(d_feature)[:n_samples]\n",
    "    y = (X @ w).ravel()\n",
    "    \n",
    "    return X, y, w\n",
    "\n",
    "\n",
    "def generate_mixed_std_basis_data(d_feature: int, n_samples_per_w: int, n_ws: int, w_cov: np.ndarray = None, noise_scale: float = 0.0) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates data y = Xw + e for multiple w vectors, where X consists of standard basis vectors.\n",
    "\n",
    "    Args:\n",
    "        d_feature: Dimension of feature space.\n",
    "        n_samples_per_w: Number of samples to generate for each w (must be <= d_feature).\n",
    "        n_ws: Number of different w vectors to use.\n",
    "        w_cov: Covariance matrix for generating w. If None, uses identity matrix.\n",
    "        noise_scale: Standard deviation of Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix of shape (n_ws * n_samples_per_w, d_feature).\n",
    "        y: Target vector of shape (n_ws * n_samples_per_w,).\n",
    "        W: Matrix of true weight vectors of shape (n_ws, d_feature).\n",
    "    \"\"\"\n",
    "    assert n_samples_per_w <= d_feature, \"n_samples_per_w must be <= d_feature for standard basis vectors\"\n",
    "    \n",
    "    if w_cov is None:\n",
    "        w_cov = np.eye(d_feature)\n",
    "    \n",
    "    W = np.random.multivariate_normal(mean=np.zeros(d_feature), cov=w_cov, size=n_ws)\n",
    "    X = np.tile(np.eye(d_feature)[:n_samples_per_w], (n_ws, 1))\n",
    "    y = np.concatenate([W[i, :n_samples_per_w] for i in range(n_ws)])\n",
    "    y += np.random.normal(scale=noise_scale, size=y.shape)\n",
    "    \n",
    "    return X, y, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $Q$ parameterization: not ensuring psd or symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training loop âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirror_descent_step(w: np.ndarray, Q: np.ndarray, lr: float, x: np.ndarray, y: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Single step of mirror descent.\n",
    "    [ 7.1: this function isn't actually used right now, since the potential update computes this manually ]\n",
    "\n",
    "    Args:\n",
    "        w: Current weight vector (d_feature, 1).\n",
    "        Q: Potential matrix (d_feature, d_feature).\n",
    "        lr: Learning rate.\n",
    "        x: Feature vector (d_feature,).\n",
    "        y: Target value.\n",
    "\n",
    "    Returns:\n",
    "        Updated weight vector (d_feature,).\n",
    "    \"\"\"\n",
    "    return w - 2 * lr * (np.inner(w, x) - y) * (Q @ x)\n",
    "\n",
    "\n",
    "def crossval(w: np.ndarray, Q: np.ndarray, lr: float, X: np.ndarray, y: np.ndarray,\n",
    "             ignore_diag=True) -> float:\n",
    "    \"\"\"\n",
    "    Perform leave-one-out cross-validation.\n",
    "\n",
    "    For each (x_i, y_i) in the dataset:\n",
    "    1. \"Train\" a model with a single step of mirror descent on (x_i, y_i)\n",
    "    2. Evaluate it on the rest of the dataset\n",
    "\n",
    "    Tested against non-vectorized version, performs identically.\n",
    "\n",
    "    Args:\n",
    "        w: Initial weight vector (d_feature,).\n",
    "        Q: Potential matrix (d_feature, d_feature).\n",
    "        lr: Learning rate.\n",
    "        X: Feature matrix (n_samples, d_feature).\n",
    "        y: Target vector (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        Average loss over all i, j with i != j.\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "\n",
    "    errors = X @ w - y  # (n_samples,)\n",
    "    XQX = X @ Q.T @ X.T  # (n_samples, n_samples)\n",
    "    L_squared = (errors - 2 * lr * errors[:, np.newaxis] * XQX)**2  # (n_samples, n_samples)\n",
    "    \n",
    "    # Enforcing i != j condition, if applicable\n",
    "    if ignore_diag:\n",
    "        np.fill_diagonal(L_squared, 0)\n",
    "        denom = 2 * n * (n-1)\n",
    "    else:\n",
    "        denom = 2 * n**2\n",
    "    \n",
    "    return np.sum(L_squared) / denom  # average over all L_ij\n",
    "\n",
    "def potential_update(w: np.ndarray, Q: np.ndarray, outer_lr: float, inner_lr: float, X: np.ndarray, y: np.ndarray,\n",
    "                     ignore_diag=True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Old version that uses an incomprehensible einsum instead of being clear.\n",
    "    Should delete once I'm confident the new version is\n",
    "\n",
    "    Update the potential matrix Q based on the derivative of the cross-validation loss.\n",
    "\n",
    "    Args:\n",
    "        w: Current weight vector (d_feature,).\n",
    "        Q: Current potential matrix (d_feature, d_feature).\n",
    "        outer_lr: Learning rate for updating Q.\n",
    "        inner_lr: Learning rate for the inner mirror descent step (Î· in the formula).\n",
    "        X: Feature matrix (n_samples, d_feature).\n",
    "        y: Target vector (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        Updated potential matrix Q (d_feature, d_feature).\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    errors = X @ w - y\n",
    "    XQX = X @ Q.T @ X.T\n",
    "    L = errors - 2 * inner_lr * errors[:, np.newaxis] * XQX\n",
    "    \n",
    "    if ignore_diag:\n",
    "        np.fill_diagonal(L, 0)\n",
    "        denom = n_samples * (n_samples - 1)\n",
    "    else:\n",
    "        denom = n_samples ** 2\n",
    "    \n",
    "    # Corrected matrix operations to match the original einsum\n",
    "    L_errors = L.T * errors  # (n_samples, n_samples)\n",
    "    update = X.T @ L_errors @ X  # (d_feature, d_feature)\n",
    "    \n",
    "    update *= -2 * inner_lr / denom\n",
    "    return Q - outer_lr * update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `potential_training_loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential_training_loop(d, n, inner_lr, outer_lr, w0, Q0, n_iters,\n",
    "                            w_cov=None, noise_scale=0., seed=None):\n",
    "    \"\"\"\n",
    "    Basic training loop.\n",
    "    Generates linear data and runs potential_update for n_iters iterations.\n",
    "\n",
    "    Returns crossvals, Qs, X, y, w_star.\n",
    "\n",
    "    **TODO: not sure \"seed\" works the way I want here to give reproducibility?\n",
    "    \"\"\"\n",
    "    if seed:\n",
    "        np.random.seed = seed\n",
    "\n",
    "    X, y, w_star = generate_linear_data(d, n, w_cov, noise_scale)\n",
    "\n",
    "    crossvals = np.zeros(n_iters)\n",
    "    Q = Q0.copy()\n",
    "    Qs = np.zeros((n_iters, d, d))\n",
    "    Qs[0] = Q.copy()\n",
    "    for i in range(n_iters):\n",
    "        crossvals[i] = crossval(w0, Q, inner_lr, X, y, ignore_diag=True)\n",
    "        Q = potential_update(w0, Q, outer_lr, inner_lr, X, y, ignore_diag=False)\n",
    "        Qs[i] = Q.copy()\n",
    "\n",
    "    return crossvals, Qs, X, y, w_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking correctness of `crossval` and `potential_update` âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test crossval and potential_update against unvectorized versions\n",
    "\n",
    "def random_experiment_setup():\n",
    "    d_feature = np.random.randint(5, 15)\n",
    "    n_samples = np.random.randint(5, 100)\n",
    "    lr = np.random.uniform(0.01, 1)\n",
    "    noise_scale = np.random.uniform(0, 1)\n",
    "\n",
    "    X, y, w_star = generate_linear_data(d_feature, n_samples, noise_scale=noise_scale)\n",
    "\n",
    "    w = np.random.randn(d_feature)\n",
    "    Q = np.random.randn(d_feature, d_feature)\n",
    "\n",
    "    return d_feature, n_samples, lr, X, y, w_star, w, Q\n",
    "\n",
    "n_tests = 15\n",
    "\n",
    "# ------------------------ Testing L_ij -----------------------------\n",
    "for _ in range(n_tests):\n",
    "    d, n, lr, X, y, w_star, w, Q = random_experiment_setup()\n",
    "    z = X @ w - y\n",
    "    M = X @ Q.T @ X.T\n",
    "    L = (z - 2 * lr * z[:, np.newaxis] * M)  # L[i,j] = L_{ij}\n",
    "\n",
    "    # check that it works\n",
    "    for i in range(min(d,n)):\n",
    "        for j in range(min(d,n)):\n",
    "            manual_calc = z[j] - 2 * lr * z[i] * X[j].T @ Q @ X[i]\n",
    "            assert np.isclose(L[i,j], manual_calc), f\"{i,j} {L[i,j], manual_calc}\"\n",
    "print(\"L_ij passed test.\")\n",
    "\n",
    "\n",
    "# ------------------------ Testing crossval -----------------------------\n",
    "def crossval_nonvec(w, Q, lr, xs, ys):\n",
    "    \"\"\"\n",
    "    For each (x_i, y_i) in zip(xs, ys):\n",
    "    1. \"Train\" a model with a single step of mirror descent on (x_i, y_i)\n",
    "    2. Evaluate it on the rest of the dataset\n",
    "    Return the average loss over all i, j with i != j.\n",
    "    \"\"\"\n",
    "    k = len(xs)\n",
    "    def L_ij(w, Q, xi, xj, yi, yj):\n",
    "        return ( w.T @ xj - 2 * lr * (w.T @ xi - yi) * (xj.T @ Q @ xi) - yj )**2\n",
    "    value = 0\n",
    "    for i, (xi, yi) in enumerate(zip(xs, ys)):\n",
    "        for j, (xj, yj) in enumerate(zip(xs, ys)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            value += L_ij(w, Q, xi, xj, yi, yj).item()\n",
    "    return value / (2 * k * (k-1))\n",
    "\n",
    "for _ in range(n_tests):\n",
    "    # all parameters are randomized over a bunch of runs\n",
    "    d_feature, n_samples, lr, X, y, w_star, w, Q = random_experiment_setup()\n",
    "\n",
    "    # Prepare inputs for crossval_nonvec\n",
    "    xs = [X[i, :].reshape(-1, 1) for i in range(n_samples)]  # list of (d_feature, 1) arrays\n",
    "    ys = y.tolist()  # list of floats\n",
    "\n",
    "    result_nonvec = crossval_nonvec(w[:, np.newaxis], Q, lr, xs, ys)\n",
    "    result_vec = crossval(w, Q, lr, X, y)\n",
    "\n",
    "    assert np.isclose(result_nonvec, result_vec) \n",
    "print(\"crossval passed test.\")\n",
    "\n",
    "\n",
    "# ------------------------ Testing potential_update -----------------------------\n",
    "def potential_update_nonvec(w, Q, outer_lr, inner_lr, xs, ys):\n",
    "    \"\"\"\n",
    "    Derivative of the cross-validation loss (as implemented in `crossval` above)\n",
    "    with respect to the matrix Q.\n",
    "\n",
    "    Returns the updated matrix.\n",
    "    \"\"\"\n",
    "    k = len(xs)\n",
    "    def cv_derivative(w, Q, lr, xi, xj, yi, yj):\n",
    "        err = (w.T @ xi - yi).item()\n",
    "        #return (w.T @ xj - 2 * lr * err * (xi.T @ Q @ xj) - yj).item() * err * np.outer(xi, xj)\n",
    "        return (w.T @ xj - 2 * lr * err * (xj.T @ Q @ xi) - yj).item() * err * (xj @ xi.T)\n",
    "    \n",
    "    update = 0\n",
    "    for i, (xi, yi) in enumerate(zip(xs, ys)):\n",
    "        for j, (xj, yj) in enumerate(zip(xs, ys)):\n",
    "            if i != j:\n",
    "                update += cv_derivative(w, Q, inner_lr, xi, xj, yi, yj)\n",
    "    update = - 2 * inner_lr * update / (k * (k - 1))\n",
    "    return Q - outer_lr * update\n",
    "\n",
    "for _ in range(n_tests):\n",
    "    d_feature, n_samples, inner_lr, X, y, w_star, w, Q = random_experiment_setup()\n",
    "    outer_lr = np.random.uniform(0.01, 1)\n",
    "\n",
    "    xs = [X[i, :].reshape(-1, 1) for i in range(n_samples)]  # list of (d_feature, 1) arrays\n",
    "    ys = y.tolist()  # list of floats\n",
    "\n",
    "    result_nonvec = potential_update_nonvec(w, Q, outer_lr, inner_lr, xs, ys)\n",
    "    result_vec = potential_update(w, Q, outer_lr, inner_lr, X, y)\n",
    "    assert np.allclose(result_nonvec, result_vec) \n",
    "print(\"potential_update passed test.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch work: how do you vectorize \"sum of outer products of rows of X\"\n",
    "\n",
    "# i.e. sum_{i,j} np.outer(x_i, x_j)\n",
    "d, n = 5, 10\n",
    "X = np.random.randn(n,d)\n",
    "\n",
    "# sum of outer products of rows, written as a loop\n",
    "sopr = np.zeros((d,d))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        sopr += np.outer(X[j], X[i])\n",
    "\n",
    "# turn the loop into a nested list comprehension: (n,d) -> (n, n, d, d), and then sum over first two dims\n",
    "expand = np.array( [[ np.outer(X[i], X[j]) for j in range(n)] for i in range(n)] )\n",
    "sopr2 = expand.sum((0,1))\n",
    "assert_allclose(sopr, sopr2)\n",
    "\n",
    "# turn into einsum: same expand step first, then sum over first two indices\n",
    "einsum_expand = np.einsum('ik,jl->ijkl', X, X)\n",
    "sopr3 = np.einsum('ik,jl->ijkl', X, X).sum((0,1))\n",
    "assert np.allclose(einsum_expand, expand)\n",
    "assert_allclose(sopr, sopr3)\n",
    "\n",
    "# next, combine expand and sum steps into a single einsum\n",
    "sopr4 = np.einsum('ik,jl->kl', X, X)\n",
    "assert_allclose(sopr, sopr4)\n",
    "\n",
    "# finally, figure out how to write this without an einsum. \n",
    "rowsums = X.T @ np.ones(n)\n",
    "sopr5 = np.outer(rowsums, rowsums)\n",
    "assert_allclose(sopr, sopr5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Known example: noiseless case, standard basis data âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_non_pd_potential_std_basis(w0: np.ndarray, w_star: np.ndarray, lr: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns matrix Q that makes the error L_[ij] = 0 for standard basis data.\n",
    "\n",
    "    Args:\n",
    "        w0: Initial weight vector (d_feature,).\n",
    "        w_star: True weight vector (d_feature,).\n",
    "        lr: Learning rate.\n",
    "\n",
    "    Returns:\n",
    "        Optimal potential matrix Q (d_feature, d_feature).\n",
    "    \"\"\"\n",
    "    d_feature = w0.shape[0]\n",
    "    Q = np.zeros((d_feature, d_feature))\n",
    "    for i in range(d_feature):\n",
    "        for j in range(d_feature):\n",
    "            Q[i, j] = ((w0[i] - w_star[i]) / (w0[j] - w_star[j])).item()\n",
    "    return Q / (2 * lr)\n",
    "\n",
    "def stationarity_condition_Q_Lij(Q, X, w, w_star, lr):\n",
    "    \"\"\"\n",
    "    Stationarity condition: <u, Qv> = <u,z> / (2Î· <v,z>), where z = w_0 - w_star.\n",
    "    \n",
    "    Given a data matrix X, this returns <X[i], QX[j]> - <X[i], z> / (2Î· <X[j], z>) for all i,j\n",
    "    \"\"\"\n",
    "    errors = X @ (w - w_star)  # prediction errors\n",
    "    error_ratio_matrix = errors[:, np.newaxis] / (2 * lr * errors)  # (n, n), matrix of values <X[i], z> / (2Î· <X[j], z>)\n",
    "    return X @ Q @ X.T - error_ratio_matrix\n",
    "\n",
    "def stationarity_condition_Q_full(Q, X, w, w_star, lr):\n",
    "    errors = X @ (w - w_star)   # (n,)\n",
    "    \n",
    "    # TODO: would be nice to replace this einsum with something legible\n",
    "    left_hand_side = np.einsum('j,ia,ab,jb,ik,jl->kl', errors**2, X, Q, X, X, X)\n",
    "\n",
    "    XTe = X.T @ errors\n",
    "    right_hand_side = np.outer(XTe, XTe)  # (d,d)\n",
    "\n",
    "    return 2 * lr * left_hand_side - right_hand_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, n = 5,5\n",
    "inner_lr, outer_lr = 1, 0.4\n",
    "n_potential_iterations = 6000\n",
    "\n",
    "X, y, w_star = generate_std_basis_data(d, n)\n",
    "w0 = np.ones((d,))\n",
    "Q0 = np.eye(d)\n",
    "#Q0 = np.random.randn(d_feature, d_feature)\n",
    "\n",
    "Q_star = optimal_non_pd_potential_std_basis(w0, w_star, inner_lr)\n",
    "\n",
    "Q = Q0.copy()\n",
    "crossvals = np.zeros(n_potential_iterations)\n",
    "Qs = np.zeros((n_potential_iterations, d, d))\n",
    "for i in range(n_potential_iterations):\n",
    "    crossvals[i] = crossval(w0, Q, inner_lr, X, y)\n",
    "    Qs[i] = Q\n",
    "    Q = potential_update(w0, Q, outer_lr, inner_lr, X, y, ignore_diag=False)\n",
    "Q_dists = np.linalg.norm(Qs - Q_star, axis=(1, 2))\n",
    "\n",
    "# Plotting results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(crossvals)\n",
    "plt.title(\"Crossval loss\")\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(Q_dists)\n",
    "plt.title(\"$d(Q, Q_\\\\star)$\")\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot failure of stationarity condition per iteration for several different dimensions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue', 'red', 'green', 'orange', 'gray', 'brown', 'purple']\n",
    "eps = 1e-8\n",
    "ignore_diag = False\n",
    "\n",
    "for size in range(3, 8):\n",
    "    d, n = size, size\n",
    "    inner_lr, outer_lr = 1, 0.4\n",
    "    n_potential_iterations = 1000 * size\n",
    "\n",
    "    #print(f\"size={size}, rep={rep}\")\n",
    "    w0 = np.ones((d,))\n",
    "    Q0 = np.eye(d)\n",
    "    \n",
    "    for rep in range(2):\n",
    "        X, y, w_star = generate_std_basis_data(d, n)    \n",
    "        Q_star = optimal_non_pd_potential_std_basis(w0, w_star, inner_lr)\n",
    "        Q = Q0.copy()\n",
    "        crossvals = []\n",
    "        Qs = []\n",
    "\n",
    "        for i in range(n_potential_iterations):\n",
    "            Q = potential_update(w0, Q, outer_lr, inner_lr, X, y, ignore_diag=False)\n",
    "            crossvals.append(crossval(w0, Q, inner_lr, X, y))\n",
    "            Qs.append(Q.copy())\n",
    "            if crossvals[-1] < eps:\n",
    "                break\n",
    "\n",
    "        crossvals = np.array(crossvals)\n",
    "        Qs = np.array(Qs)\n",
    "        #print(f\"  final crossval: {crossvals[-1]}\")\n",
    "        \n",
    "        j = size-3\n",
    "        sp_diffs = np.array([stationarity_condition_Q_Lij(Q, X, w0, w_star, inner_lr) for Q in Qs])\n",
    "        mean_abs_diffs = np.mean(np.abs(sp_diffs), axis=(1,2))\n",
    "        plt.plot(mean_abs_diffs, color=f\"{colors[j]}\", label=f\"d={size}, rep={rep+1}\")\n",
    "        \n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.axhline(0, color=\"black\")\n",
    "plt.ylabel(\"Stationary point difference\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.title(\"Stationarity condition vs iteration for different dimensions, std basis data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot failure of stationarity condition at $\\epsilon$ convergence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.random.randn(n, d)\n",
    "\n",
    "sp_diffs = np.array([stationarity_condition_Q_Lij(Q, X, w0, w_star, lr) for Q in Qs])\n",
    "sp_diffs_val = np.array([stationarity_condition_Q_Lij(Q, X_val, w0, w_star, lr) for Q in Qs])\n",
    "\n",
    "mean_abs_diffs = np.mean(np.abs(sp_diffs), axis=(1,2))\n",
    "max_abs_diffs = np.max(np.abs(sp_diffs), axis=(1,2))\n",
    "min_abs_diffs = np.min(np.abs(sp_diffs), axis=(1,2))\n",
    "\n",
    "mean_abs_diffs_val = np.mean(np.abs(sp_diffs_val), axis=(1,2))\n",
    "max_abs_diffs_val= np.max(np.abs(sp_diffs_val), axis=(1,2))\n",
    "min_abs_diffs_val= np.min(np.abs(sp_diffs_val), axis=(1,2))\n",
    "\n",
    "#plt.plot(max_abs_diffs)\n",
    "plt.plot(mean_abs_diffs)\n",
    "plt.plot(mean_abs_diffs_val)\n",
    "#plt.plot(min_abs_diffs)\n",
    "#plt.yscale('log')\n",
    "plt.title(\"$L_{ij}$ stationarity condition on std basis data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.random.randn(n, d)\n",
    "\n",
    "sp_diffs = np.array([stationarity_condition_Q_full(Q, X, w0, w_star, lr) for Q in Qs])\n",
    "sp_diffs_val = np.array([stationarity_condition_Q_full(Q, X_val, w0, w_star, lr) for Q in Qs])\n",
    "\n",
    "mean_abs_diffs = np.mean(np.abs(sp_diffs), axis=(1,2))\n",
    "max_abs_diffs = np.max(np.abs(sp_diffs), axis=(1,2))\n",
    "min_abs_diffs = np.min(np.abs(sp_diffs), axis=(1,2))\n",
    "\n",
    "mean_abs_diffs_val = np.mean(np.abs(sp_diffs_val), axis=(1,2))\n",
    "max_abs_diffs_val= np.max(np.abs(sp_diffs_val), axis=(1,2))\n",
    "min_abs_diffs_val= np.min(np.abs(sp_diffs_val), axis=(1,2))\n",
    "\n",
    "#plt.plot(max_abs_diffs)\n",
    "plt.plot(mean_abs_diffs, label=\"train\")\n",
    "#plt.plot(mean_abs_diffs_val, label=\"val\")\n",
    "plt.legend()\n",
    "#plt.plot(min_abs_diffs)\n",
    "#plt.yscale('log')\n",
    "plt.title(\"Full stationarity condition on std basis data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = w0 - w_star\n",
    "z.shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_closed_form(X, w, w_star, lr):\n",
    "    z = w - w_star\n",
    "    E = np.diag(X @ z)**2\n",
    "    return np.outer(z, z) @ X.T @ X @ np.linalg.inv(X.T @ E @ X) / (2 * lr)\n",
    "\n",
    "\n",
    "# Check that it works the way it should\n",
    "X_cov = np.eye(d)\n",
    "for i in range(3):\n",
    "    X_cov[i,i] *= 1000\n",
    "\n",
    "d, n = 10, 100\n",
    "lr = 0.5\n",
    "X = np.random.multivariate_normal(mean=np.zeros(d), cov=X_cov, size=(n,))\n",
    "w0 = np.random.randn(d)\n",
    "w_star = np.ones(d)\n",
    "\n",
    "Q_star = stationary_closed_form(X, w0, w_star, lr)\n",
    "plt.imshow(Q_star)\n",
    "plt.grid(0)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# pretty relaxed atol due to matrix inverse, sometimes using poorly-conditioned X_cov\n",
    "assert_allclose(stationarity_condition_Q_full(Q_star, X, w0, w_star, lr), 0, atol=1e-3, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea here: resample X a bunch of times with same cov (with distinctive subspace)\n",
    "# and then check if the top eigenvectors are aligned with each other, or something\n",
    "#  or generally, what properties of the resulting `stationary_closed_form` matrix are preserved / similar\n",
    "#\n",
    "# eigenvector thing seems mostly negative. a bit complicated by complex values, not sure what to make of that.\n",
    "\n",
    "X_cov = np.eye(d)\n",
    "for i in range(3):\n",
    "    X_cov[i,i] *= 1000\n",
    "\n",
    "d, n = 10, 100\n",
    "lr = 0.5\n",
    "w_star = np.ones(d)\n",
    "\n",
    "Qs, Xs, ws = [], [], []\n",
    "eigenvecs = []\n",
    "for i in range(8):\n",
    "    Xs.append(np.random.multivariate_normal(mean=np.zeros(d), cov=X_cov, size=(n,)))\n",
    "    ws.append(np.random.randn(d))\n",
    "    Qs.append(stationary_closed_form(Xs[-1], ws[-1], w_star, lr))\n",
    "\n",
    "    eigs = np.linalg.eig(Qs[-1])\n",
    "    absvals = np.abs(eigs.eigenvalues)\n",
    "    eigindex = np.argmax(absvals)\n",
    "    eigenvecs.append(eigs.eigenvectors[eigindex])\n",
    "\n",
    "eigenvecs = np.array(eigenvecs)\n",
    "xxT = eigenvecs @ np.conj(eigenvecs).T\n",
    "np.fill_diagonal(xxT, 0)\n",
    "plt.imshow(np.abs(xxT))  # i want to know if other vectors are aligned\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# for i, Q in enumerate(Qs):\n",
    "\n",
    "#     plt.scatter(range(d), absvals)\n",
    "#     plt.title(f\"{eigindex}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs for single $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "d,n = 5, 50\n",
    "inner_lr, outer_lr = 0.1, 0.02\n",
    "n_iters = 3500\n",
    "w_cov = np.eye(d)\n",
    "w0 = np.ones(d)\n",
    "Q0 = np.eye(d)\n",
    "\n",
    "# Training loop\n",
    "crossvals, Qs, X, y, w_star = potential_training_loop(d, n, inner_lr, outer_lr, w0, Q0, n_iters, w_cov, seed=123)\n",
    "\n",
    "# Plot crossvals\n",
    "plt.plot(crossvals)\n",
    "plt.title(f\"CV loss: d={d}, n={n}\")\n",
    "#plt.yscale('log')\n",
    "\n",
    "plot_matrix_evolution(Qs, main_title=f\"Evolution of $Q$ during training: d={d}, n={n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to a single step of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_final = Qs[-1]\n",
    "lr = inner_lr\n",
    "\n",
    "trained_md_crossvals = []\n",
    "gd_crossvals = []\n",
    "X_vals = []\n",
    "for i in np.arange(1500):\n",
    "    X_val = np.random.randn(n, d)\n",
    "    y_val = X_val @ w_star\n",
    "    trained_md_crossvals.append(crossval(w0, Q_final, lr, X_val, y_val))\n",
    "    gd_crossvals.append(crossval(w0, np.eye(d), lr, X_val, y_val))\n",
    "    X_vals.append(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_gd_crossvals = np.array([trained_md_crossvals, gd_crossvals])\n",
    "sorted_indices = np.argsort(md_gd_crossvals[0])                        # sort by value of md crossval\n",
    "sorted_indices = np.argsort(md_gd_crossvals[0] - md_gd_crossvals[1])   # sort by difference between crossvals\n",
    "md_gd_crossvals = md_gd_crossvals[:, sorted_indices]\n",
    "X_vals = np.array(X_vals)[sorted_indices]\n",
    "\n",
    "plt.plot(md_gd_crossvals[0], label=\"md\")\n",
    "plt.plot(md_gd_crossvals[1], label=\"gd\")\n",
    "plt.plot(md_gd_crossvals[1] - md_gd_crossvals[0], alpha=0.5, linestyle='--', color='gray', label=\"diffs\")\n",
    "plt.axhline(y=np.mean(md_gd_crossvals[0]), label=\"md avg\", color='blue', linestyle='--')\n",
    "plt.axhline(y=np.mean(md_gd_crossvals[1]), label=\"gd avg\", color='green', linestyle='--')\n",
    "plt.ylabel(\"Cross-validation loss over validation datasets\")\n",
    "plt.title(\"Mirror descent (blue) vs gradient descent (green).\\nSorted by md crossval\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of next cell: Investigating `X_val` statistics across runs.**\n",
    "\n",
    "`-----------------There's stuff here that I'd like to follow up on-------------------`\n",
    "\n",
    "Typically, what I see above is that mirror descent does better (yay!).\n",
    "\n",
    "Sometimes it seems to do worse, consistently -- not sure if this was an error in the code, a bad $Q$, or what.\n",
    "\n",
    "And sometimes it seems to do better most of the time, maybe 90%, but then do worse sometimes. I (i.e. Claude) wrote the code below to investigate whether, in these cases, the `X_val` data that md performs better on is systematically different from the data that gd performs better on.\n",
    "\n",
    "I did a 3-minute training run for $Q$ to make sure I was working with something good, and then had a consistently good gap between md and gd over many iterations -- they never swapped places. So maybe it was an undertraining issue the whole time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Find the split point where md_gd_crossvals[0,k] >= md_gd_crossvals[1,k]\n",
    "split_point = np.argmax(md_gd_crossvals[0] >= md_gd_crossvals[1])\n",
    "\n",
    "# Calculate various statistics\n",
    "means = np.mean(X_vals, axis=(1, 2))\n",
    "mean_distances = np.abs(means)\n",
    "cov_matrices = np.array([np.cov(X.T) for X in X_vals])\n",
    "top_eigenvalues = np.array([np.linalg.eigvals(cov).max() for cov in cov_matrices])\n",
    "condition_numbers = np.array([np.linalg.cond(cov) for cov in cov_matrices])\n",
    "std_devs = np.std(X_vals, axis=(1, 2))\n",
    "skewness = np.mean(((X_vals - np.mean(X_vals, axis=(1, 2), keepdims=True)) / \n",
    "                    np.std(X_vals, axis=(1, 2), keepdims=True))**3, axis=(1, 2))\n",
    "kurtosis = np.mean(((X_vals - np.mean(X_vals, axis=(1, 2), keepdims=True)) / \n",
    "                    np.std(X_vals, axis=(1, 2), keepdims=True))**4, axis=(1, 2)) - 3\n",
    "ks_stats = np.array([stats.kstest(X.flatten(), 'norm').statistic for X in X_vals])\n",
    "\n",
    "# Calculate the difference between the two runs\n",
    "run_differences = X_vals[1] - X_vals[0]\n",
    "diff_means = np.mean(run_differences, axis=1)\n",
    "\n",
    "# Plotting function for line plots\n",
    "def plot_metric(ax, metric, title):\n",
    "    ax.plot(metric)\n",
    "    ax.set_title(title)\n",
    "    ax.axvline(x=split_point, color='r', linestyle='--', label=f'Split point (k={split_point})')\n",
    "    ax.legend()\n",
    "\n",
    "# Create line plots\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n",
    "plot_metric(axs[0, 0], mean_distances, 'Mean Distances from Zero')\n",
    "plot_metric(axs[0, 1], top_eigenvalues, 'Top Eigenvalues')\n",
    "plot_metric(axs[0, 2], condition_numbers, 'Condition Numbers')\n",
    "plot_metric(axs[1, 0], std_devs, 'Standard Deviations')\n",
    "plot_metric(axs[1, 1], skewness, 'Skewness')\n",
    "plot_metric(axs[1, 2], kurtosis, 'Kurtosis')\n",
    "plot_metric(axs[2, 0], ks_stats, 'KS Statistics')\n",
    "plot_metric(axs[2, 1], diff_means, 'Mean Differences Between Runs')\n",
    "\n",
    "# Plot md_gd_crossvals\n",
    "axs[2, 2].plot(md_gd_crossvals[0], label='First run')\n",
    "axs[2, 2].plot(md_gd_crossvals[1], label='Second run')\n",
    "axs[2, 2].axvline(x=split_point, color='r', linestyle='--', label=f'Split point (k={split_point})')\n",
    "axs[2, 2].set_title('md_gd_crossvals')\n",
    "axs[2, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Function to plot overlapping histograms\n",
    "def plot_overlapping_histogram(data, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data[:split_point], bins=20, alpha=0.5, label='Before split')\n",
    "    plt.hist(data[split_point:], bins=20, alpha=0.5, label='After split')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# # Plot overlapping histograms for each metric\n",
    "# metrics = [mean_distances, top_eigenvalues, condition_numbers, std_devs, skewness, kurtosis, ks_stats, diff_means]\n",
    "# metric_names = ['Mean Distances', 'Top Eigenvalues', 'Condition Numbers', 'Standard Deviations', \n",
    "#                 'Skewness', 'Kurtosis', 'KS Statistics', 'Mean Differences Between Runs']\n",
    "\n",
    "# for metric, name in zip(metrics, metric_names):\n",
    "#     plot_overlapping_histogram(metric, name)\n",
    "\n",
    "# # Print summary statistics for before and after split point\n",
    "# def print_summary(metric, name):\n",
    "#     print(f\"\\n{name}:\")\n",
    "#     print(f\"Before split: mean = {np.mean(metric[:split_point]):.4f}, std = {np.std(metric[:split_point]):.4f}\")\n",
    "#     print(f\"After split: mean = {np.mean(metric[split_point:]):.4f}, std = {np.std(metric[split_point:]):.4f}\")\n",
    "\n",
    "# for metric, name in zip(metrics, metric_names):\n",
    "#     print_summary(metric, name)\n",
    "\n",
    "# Print the split point\n",
    "print(f\"\\nSplit point (k) where md_gd_crossvals[0,k] >= md_gd_crossvals[1,k]: {split_point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs for mixed $w$ / recovering covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "* Are there consistent, visible patterns in $Q$?\n",
    "* What properties are preseved between multiple runs with different data?\n",
    "* How does $\\Sigma_w$ itself perform as a mirror map?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic \"run experiment\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential_training_loop_mixed_w(d, n_samples_per_w, n_ws, inner_lr, outer_lr, w0, Q0, n_iters,\n",
    "                                    w_cov=None, noise_scale=0.):\n",
    "    \n",
    "    X, y, W_stars = generate_mixed_linear_data(d, n_samples_per_w, n_ws, w_cov, noise_scale)\n",
    "\n",
    "    Q = Q0.copy()\n",
    "    crossvals = np.zeros(n_iters)\n",
    "    Qs = np.zeros((n_iters, d, d))\n",
    "    for i in range(n_iters):\n",
    "        Qs[i] = Q.copy()\n",
    "        crossvals[i] = crossval(w0, Q, inner_lr, X, y, ignore_diag=True)\n",
    "        Q = potential_update(w0, Q, outer_lr, inner_lr, X, y, ignore_diag=False)\n",
    "\n",
    "    return crossvals, Qs, X, y, W_stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-rank $\\Sigma_w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other parameters for training\n",
    "d, nspw, n_ws = 8, 3, 25  # nspw = n_samples_per_w\n",
    "inner_lr, outer_lr = 0.3, 0.005\n",
    "n_iters = 700\n",
    "w0 = np.ones(d)\n",
    "Q0 = np.eye(d)\n",
    "\n",
    "# setting up w_cov\n",
    "rank = 2\n",
    "assert rank < d, f\"rank {rank} should be less than dimension {d}\"\n",
    "w_cov = np.diag([1]*rank + [0]*(d-rank))\n",
    "\n",
    "# Train potential\n",
    "crossvals, Qs, X, y, W_stars = potential_training_loop_mixed_w(d, nspw, n_ws, inner_lr, outer_lr, w0, Q0, n_iters, w_cov)\n",
    "\n",
    "# Plot crossvals\n",
    "plt.plot(crossvals)\n",
    "plt.title(f\"CV loss: d={d}, n={nspw * n_ws}, rank($\\Sigma_w$)={rank}\")\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot images of Q iterates over time\n",
    "plot_matrix_evolution(Qs, extra_matrix=w_cov, extra_matrix_title=f'w_cov, rank={rank}', main_title='Evolution of $Q$ during training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poorly-conditioned $\\Sigma_w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationary-Point Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L_{ij}$ stationary point condition: $x_j^\\top Q x_i = z_j / (2 \\eta z_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.random.randn(n, d)\n",
    "\n",
    "sp_diffs = np.array([stationarity_condition_Q_Lij(Q, X, w0, w_star, lr) for Q in Qs])\n",
    "sp_diffs_val = np.array([stationarity_condition_Q_Lij(Q, X_val, w0, w_star, lr) for Q in Qs])\n",
    "\n",
    "mean_abs_diffs = np.mean(np.abs(sp_diffs), axis=(1,2))\n",
    "max_abs_diffs = np.max(np.abs(sp_diffs), axis=(1,2))\n",
    "min_abs_diffs = np.min(np.abs(sp_diffs), axis=(1,2))\n",
    "\n",
    "mean_abs_diffs_val = np.mean(np.abs(sp_diffs_val), axis=(1,2))\n",
    "max_abs_diffs_val= np.max(np.abs(sp_diffs_val), axis=(1,2))\n",
    "min_abs_diffs_val= np.min(np.abs(sp_diffs_val), axis=(1,2))\n",
    "\n",
    "#plt.plot(max_abs_diffs)\n",
    "plt.plot(mean_abs_diffs, label=\"train\")\n",
    "plt.plot(mean_abs_diffs_val, label=\"val\")\n",
    "plt.legend()\n",
    "#plt.plot(min_abs_diffs)\n",
    "plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-energy stationary-point condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$2\\eta \\sum_{i,j} \\varepsilon_j (x_i^\\top Q x_j) x_{ik}x_{j\\ell}  = \\sum_{i,j} \\varepsilon_i \\varepsilon_j x_{ik} x_{j\\ell}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.random.randn(n, d)\n",
    "\n",
    "sp_diffs = np.array([stationarity_condition_Q_full(Q, X, w0, w_star, lr) for Q in Qs])\n",
    "sp_diffs_val = np.array([stationarity_condition_Q_full(Q, X_val, w0, w_star, lr) for Q in Qs])\n",
    "\n",
    "mean_abs_diffs = np.mean(np.abs(sp_diffs), axis=(1,2))\n",
    "max_abs_diffs = np.max(np.abs(sp_diffs), axis=(1,2))\n",
    "min_abs_diffs = np.min(np.abs(sp_diffs), axis=(1,2))\n",
    "\n",
    "mean_abs_diffs_val = np.mean(np.abs(sp_diffs_val), axis=(1,2))\n",
    "max_abs_diffs_val= np.max(np.abs(sp_diffs_val), axis=(1,2))\n",
    "min_abs_diffs_val= np.min(np.abs(sp_diffs_val), axis=(1,2))\n",
    "\n",
    "#plt.plot(max_abs_diffs)\n",
    "plt.plot(mean_abs_diffs, label=\"train\")\n",
    "plt.plot(mean_abs_diffs_val, label=\"val\")\n",
    "plt.legend()\n",
    "#plt.plot(min_abs_diffs)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_abs_diffs[:500], label=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $UU^\\top$ parameterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirror_descent_step_U(w: np.ndarray, U: np.ndarray, lr: float, x: np.ndarray, y: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Single step of mirror descent.\n",
    "    Just calls `mirror_descent_step` with `Q = U @ U.T`.\n",
    "\n",
    "    [ 7.1: this function isn't actually used right now, since the potential update computes this manually ]\n",
    "\n",
    "    Args:\n",
    "        w: Current weight vector (d_feature, 1).\n",
    "        U: Factor of potential matrix: Q = U @ U.T\n",
    "        lr: Learning rate.\n",
    "        x: Feature vector (d_feature,).\n",
    "        y: Target value.\n",
    "\n",
    "    Returns:\n",
    "        Updated weight vector (d_feature,).\n",
    "    \"\"\"\n",
    "    return mirror_descent_step(w, U @ U.T, lr, x, y)\n",
    "\n",
    "\n",
    "def crossval_U(w: np.ndarray, U: np.ndarray, lr: float, X: np.ndarray, y: np.ndarray,\n",
    "             ignore_diag=True) -> float:\n",
    "    \"\"\"\n",
    "    Perform leave-one-out cross-validation.\n",
    "    Just calls `crossval` with `Q = U @ U.T`.\n",
    "\n",
    "    For each (x_i, y_i) in the dataset:\n",
    "    1. \"Train\" a model with a single step of mirror descent on (x_i, y_i)\n",
    "    2. Evaluate it on the rest of the dataset\n",
    "\n",
    "    Tested against non-vectorized version, performs identically.\n",
    "\n",
    "    Args:\n",
    "        w: Initial weight vector (d_feature,).\n",
    "        U: Factor of potential matrix: Q = U @ U.T\n",
    "        lr: Learning rate.\n",
    "        X: Feature matrix (n_samples, d_feature).\n",
    "        y: Target vector (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        Average loss over all i, j with i != j.\n",
    "    \"\"\"\n",
    "    return crossval(w, U @ U.T, lr, X, y, ignore_diag)\n",
    "\n",
    "def potential_update_U(w: np.ndarray, U: np.ndarray, outer_lr: float, inner_lr: float, X: np.ndarray, y: np.ndarray,\n",
    "                       ignore_diag=True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Update the potential matrix U based on the derivative of the cross-validation loss, where Q = UU^T.\n",
    "\n",
    "    Args:\n",
    "        w: Current weight vector (d_feature,).\n",
    "        U: Current potential matrix factor (d_feature, d_feature).\n",
    "        outer_lr: Learning rate for updating U.\n",
    "        inner_lr: Learning rate for the inner mirror descent step (Î· in the formula).\n",
    "        X: Feature matrix (n_samples, d_feature).\n",
    "        y: Target vector (n_samples,).\n",
    "        ignore_diag: Whether to ignore diagonal terms in the loss calculation.\n",
    "\n",
    "    Returns:\n",
    "        Updated potential matrix factor U (d_feature, d_feature).\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    denom = n_samples ** 2\n",
    "    \n",
    "    errors = X @ w - y\n",
    "    L = errors - 2 * inner_lr * errors[:, np.newaxis] * X @ U @ U.T @ X.T\n",
    "    \n",
    "    if ignore_diag:\n",
    "        np.fill_diagonal(L, 0)\n",
    "        denom -= n_samples\n",
    "\n",
    "    L_errors = L * errors[:, np.newaxis]  # (n_samples, n_samples)\n",
    "    update = X.T @ (L_errors + L_errors.T) @ X @ U\n",
    "    \n",
    "    # Apply scaling factor\n",
    "    update *= -2 * inner_lr / denom\n",
    "    assert update.shape == U.shape, f\"Potential update of shape {update.shape} is incompatible with U shape of {U.shape}.\"\n",
    "    \n",
    "    # Update U using gradient descent\n",
    "    return U - outer_lr * update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential_training_loop_U(d, n, inner_lr, outer_lr, w0, U0, n_iters,\n",
    "                            w_cov=None, noise_scale=0., seed=None):\n",
    "    \"\"\"\n",
    "    Basic training loop.\n",
    "    Generates linear data and runs potential_update for n_iters iterations.\n",
    "\n",
    "    Returns crossvals, Qs, X, y, w_star.\n",
    "    \"\"\"\n",
    "    X, y, w_star = generate_linear_data(d, n, w_cov, noise_scale)\n",
    "\n",
    "    Us = [U0]\n",
    "    crossvals = [crossval_U(w0, U0, inner_lr, X, y, ignore_diag=True)]\n",
    "    U = U0.copy()\n",
    "    for i in range(n_iters):\n",
    "        U = potential_update_U(w0, U, outer_lr, inner_lr, X, y, ignore_diag=False)\n",
    "        Us.append(U)\n",
    "        crossvals.append(crossval_U(w0, U, inner_lr, X, y, ignore_diag=True))\n",
    "\n",
    "    Us = np.array(Us)\n",
    "    crossvals = np.array(crossvals)\n",
    "\n",
    "    return crossvals, Us, X, y, w_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude's numerical correctness check for `potential_update_U` \n",
    "## **TODO: VERIFY THAT THIS MAKES SENSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import eigvalsh\n",
    "\n",
    "# TODO: check that this is the same as `crossval`, and replace?\n",
    "def compute_cv_loss(w, U, inner_lr, X, y):\n",
    "    \"\"\"Compute the cross-validation loss.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    errors = X @ w - y\n",
    "    XUUTX = X @ U @ U.T @ X.T\n",
    "    L = errors[:, np.newaxis] - 2 * inner_lr * errors * XUUTX\n",
    "    np.fill_diagonal(L, 0)\n",
    "    return np.sum(L**2) / (2 * n_samples * (n_samples - 1))\n",
    "\n",
    "def numerical_gradient(w, U, inner_lr, X, y, epsilon=1e-8):\n",
    "    \"\"\"Compute numerical gradient of CV loss with respect to U.\"\"\"\n",
    "    grad = np.zeros_like(U)\n",
    "    for i in range(U.shape[0]):\n",
    "        for j in range(U.shape[1]):\n",
    "            U_plus = U.copy()\n",
    "            U_plus[i, j] += epsilon\n",
    "            U_minus = U.copy()\n",
    "            U_minus[i, j] -= epsilon\n",
    "            grad[i, j] = (compute_cv_loss(w, U_plus, inner_lr, X, y) - \n",
    "                          compute_cv_loss(w, U_minus, inner_lr, X, y)) / (2 * epsilon)\n",
    "    return grad\n",
    "\n",
    "def test_potential_update_U():\n",
    "    # Generate random data\n",
    "    n_samples, d_feature = 20, 5\n",
    "    X = np.random.randn(n_samples, d_feature)\n",
    "    w = np.random.randn(d_feature)\n",
    "    y = X @ w + np.random.randn(n_samples) * 0.1\n",
    "    U = np.random.randn(d_feature, d_feature)\n",
    "    \n",
    "    # Set learning rates\n",
    "    outer_lr, inner_lr = 0.01, 0.1\n",
    "    \n",
    "    # Compute update using our function\n",
    "    U_new = potential_update_U(w, U, outer_lr, inner_lr, X, y)\n",
    "    actual_update = (U_new - U) / outer_lr\n",
    "    \n",
    "    # Compute numerical gradient\n",
    "    numerical_grad = numerical_gradient(w, U, inner_lr, X, y)\n",
    "    \n",
    "    # Compare the results\n",
    "    assert_allclose(actual_update, -numerical_grad, rtol=1e-4, atol=1e-4,\n",
    "                    err_msg=\"Gradient from potential_update_U doesn't match numerical gradient\")\n",
    "    \n",
    "    # Check if the resulting Q is positive semidefinite\n",
    "    Q_new = U_new @ U_new.T\n",
    "    min_eigenvalue = eigvalsh(Q_new).min()\n",
    "    assert min_eigenvalue >= -1e-10, f\"Resulting Q is not positive semidefinite. Min eigenvalue: {min_eigenvalue}\"\n",
    "    \n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_potential_update_U()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs for single $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "certified_random_matrix = np.random.randn(d, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "d,n = 5, 100\n",
    "inner_lr, outer_lr = 0.1, 0.01\n",
    "n_iters = 1000\n",
    "w_cov = np.eye(d)\n",
    "w0 = np.ones(d)\n",
    "U0 = certified_random_matrix\n",
    "\n",
    "# Training loop\n",
    "crossvals, Us, X, y, w_star = potential_training_loop_U(d, n, inner_lr, outer_lr, w0, U0, n_iters, w_cov)\n",
    "\n",
    "UUTs = np.array([U @ U.T for U in Us])\n",
    "\n",
    "# Plot crossvals\n",
    "plt.plot(crossvals)\n",
    "plt.title(f\"CV loss: d={d}, n={n}\")\n",
    "plt.yscale('log')\n",
    "\n",
    "plot_matrix_evolution(Us, main_title=f\"Evolution of $U$ during training: d={d}, n={n}\")\n",
    "plot_matrix_evolution(UUTs, main_title=f\"Evolution of $UU^\\\\top$ during training: d={d}, n={n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix_evolution(UUTs[250:], 5)\n",
    "plot_matrix_evolution(Us - Us.transpose(0,2,1), 5, main_title=\"Asymmetry during training: $U - U^\\\\top$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs for mixed $w$ / recovering covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
